{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":38257,"databundleVersionId":4319132,"sourceType":"competition"},{"sourceId":4935931,"sourceType":"datasetVersion","datasetId":2862267},{"sourceId":5389852,"sourceType":"datasetVersion","datasetId":3120713},{"sourceId":5390720,"sourceType":"datasetVersion","datasetId":3124889},{"sourceId":6286441,"sourceType":"datasetVersion","datasetId":3615022},{"sourceId":6655460,"sourceType":"datasetVersion","datasetId":3474549},{"sourceId":117945598,"sourceType":"kernelVersion"}],"dockerImageVersionId":30446,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport random\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\nfrom sklearn.preprocessing import RobustScaler","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:08:30.681883Z","iopub.execute_input":"2023-11-16T10:08:30.682493Z","iopub.status.idle":"2023-11-16T10:08:31.759911Z","shell.execute_reply.started":"2023-11-16T10:08:30.682456Z","shell.execute_reply":"2023-11-16T10:08:31.758869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nfrom torch.nn import ModuleList, Linear, BatchNorm1d\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:08:31.762068Z","iopub.execute_input":"2023-11-16T10:08:31.762417Z","iopub.status.idle":"2023-11-16T10:08:40.625429Z","shell.execute_reply.started":"2023-11-16T10:08:31.762382Z","shell.execute_reply":"2023-11-16T10:08:40.624301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    !pip install --no-index --no-deps /kaggle/input/torch-geometric-gpu/wheelhouse/*.whl\nelse:\n    !pip install --no-index --no-deps /kaggle/input/torch-geometric-cpu/wheelhouse/*.whl","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:08:40.626948Z","iopub.execute_input":"2023-11-16T10:08:40.627325Z","iopub.status.idle":"2023-11-16T10:08:46.907160Z","shell.execute_reply.started":"2023-11-16T10:08:40.627288Z","shell.execute_reply":"2023-11-16T10:08:46.905707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm  -r software\n!scp -r /kaggle/input/graphnet-and-dependencies/software .","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:08:46.911495Z","iopub.execute_input":"2023-11-16T10:08:46.912478Z","iopub.status.idle":"2023-11-16T10:09:20.667574Z","shell.execute_reply.started":"2023-11-16T10:08:46.912430Z","shell.execute_reply":"2023-11-16T10:09:20.665830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd software/graphnet;pip install --no-index --find-links=\"/kaggle/working/software/dependencies\" -e .[torch]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:09:20.669460Z","iopub.execute_input":"2023-11-16T10:09:20.669820Z","iopub.status.idle":"2023-11-16T10:40:19.834512Z","shell.execute_reply.started":"2023-11-16T10:09:20.669782Z","shell.execute_reply":"2023-11-16T10:40:19.833351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch_cluster\nimport torch_geometric\nimport torch_geometric.nn as pyg_nn\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import EdgeConv, global_add_pool, global_mean_pool, global_max_pool\nfrom torch_geometric.nn.pool import knn_graph\nfrom torch_geometric.typing import Adj\nfrom torch_geometric.utils.homophily import homophily\n\nfrom torch_scatter import scatter_max, scatter_mean, scatter_min, scatter_sum\n\n\nINPUT_PATH = \"/kaggle/input/icecube-neutrinos-in-deep-ice\"\nTRANSPARENCY_PATH = \"/kaggle/input/icecubetransparency/ice_transparency.txt\"\nMODEL_PATH = \"/kaggle/input/gnn-ready-model/EdgeConvV10.pth\"\nMODE = 'resume'\nTRAIN_BATCHES = 3\n\nGLOBAL_POOLINGS = {\n    \"max\": global_max_pool,\n    \"add\": global_add_pool,\n    \"mean\": global_mean_pool,\n}\n\n_dtype = {\n    \"batch_id\": \"int16\",\n    \"event_id\": \"int64\",\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:19.836479Z","iopub.execute_input":"2023-11-16T10:40:19.837425Z","iopub.status.idle":"2023-11-16T10:40:20.496208Z","shell.execute_reply.started":"2023-11-16T10:40:19.837371Z","shell.execute_reply":"2023-11-16T10:40:20.495057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting seed = 0 for reproducibility","metadata":{}},{"cell_type":"code","source":"def seed_settings(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_settings(seed=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:20.497724Z","iopub.execute_input":"2023-11-16T10:40:20.498118Z","iopub.status.idle":"2023-11-16T10:40:20.510961Z","shell.execute_reply.started":"2023-11-16T10:40:20.498076Z","shell.execute_reply":"2023-11-16T10:40:20.510046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"markdown","source":"### Making features using ice transparency data","metadata":{}},{"cell_type":"code","source":"# Ice Data Taken from Measurement of South Pole ice transparency with the IceCube\n# LED calibration system https://arxiv.org/pdf/1301.5361.pdf\n!cat /kaggle/input/icecubetransparency/ice_transparency.txt","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:20.512314Z","iopub.execute_input":"2023-11-16T10:40:20.513164Z","iopub.status.idle":"2023-11-16T10:40:21.537973Z","shell.execute_reply.started":"2023-11-16T10:40:20.513121Z","shell.execute_reply":"2023-11-16T10:40:21.536579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ice_transparency(data_path, center_depth=1950):\n    \"\"\"\n    Calculates normalized ice scattering and absorption for all depths,\n    using data from Measurement of South Pole ice transparency with the IceCube\n    LED calibration system\n\n    Center_depth is center of IceCube underground array\n    \"\"\"\n    sensor_amount = 500\n\n    df = pd.read_csv(data_path, delim_whitespace=True)\n    df[\"z_score\"] = (df[\"depth\"] - center_depth) / sensor_amount\n    df[[\"scattering_len_norm\", \"absorption_len_norm\"]] = RobustScaler().fit_transform(\n        df[[\"scattering_len\", \"absorption_len\"]]\n    )\n\n    # These are both roughly equivalent after scaling\n    # TODO: Make new feature (1-(sc+ab)/2)?\n    f_scattering = interp1d(df[\"z_score\"], df[\"scattering_len_norm\"])\n    f_absorption = interp1d(df[\"z_score\"], df[\"absorption_len_norm\"])\n    return f_scattering, f_absorption","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:21.539786Z","iopub.execute_input":"2023-11-16T10:40:21.540109Z","iopub.status.idle":"2023-11-16T10:40:21.548676Z","shell.execute_reply.started":"2023-11-16T10:40:21.540073Z","shell.execute_reply":"2023-11-16T10:40:21.547593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Dataset class","metadata":{}},{"cell_type":"code","source":"class IceCubeDataset(Dataset):\n    \"\"\"\n    Used to transfer data from table format to graph format, with each event\n    represented by singular graph\n    \"\"\"\n    def __init__(\n        self,\n        batch_id,\n        event_ids,\n        sensor_df,\n        mode=\"test\",\n        y=None,\n        pulse_limit=220,  # >95% events have 220 pulses or less\n    ):\n        super().__init__()\n        self.y = y\n        self.event_ids = event_ids\n        self.batch_df = pd.read_parquet(f\"{INPUT_PATH}/{mode}/batch_{batch_id}.parquet\")\n        self.sensor_df = sensor_df\n        self.pulse_limit = pulse_limit\n        self.f_scattering, self.f_absorption = ice_transparency(TRANSPARENCY_PATH)\n        # Rescaling parameters\n        self.batch_df[\"time\"] = (self.batch_df[\"time\"] - 1.0e04) / 3.0e4\n        self.batch_df[\"charge\"] = np.log10(self.batch_df[\"charge\"]) / 3.0\n        self.batch_df[\"auxiliary\"] = self.batch_df[\"auxiliary\"].astype(int) - 0.5\n\n    def len(self):\n        return len(self.event_ids)\n\n    def get(self, idx):\n        \"\"\"\n        Returns data for given event index\n        \"\"\"\n        event_id = self.event_ids[idx]\n        event = self.batch_df.loc[event_id]\n\n        # represent each event by a single graph\n        event = pd.merge(event, self.sensor_df, on=\"sensor_id\")\n        col = [\"x\", \"y\", \"z\", \"time\", \"charge\", \"auxiliary\"]\n\n        x = event[col].values.astype(float)\n        x = torch.tensor(x, dtype=torch.float32)\n        # A data object describing a homogeneous graph\n        data = Data(x=x, n_pulses=torch.tensor(x.shape[0], dtype=torch.int32))\n\n        # Downsample the large events\n        if data.n_pulses > self.pulse_limit:\n            data.x = data.x[np.random.choice(data.n_pulses, self.pulse_limit)]\n            data.n_pulses = torch.tensor(self.pulse_limit, dtype=torch.int32)\n\n        # Builds graph from the k-nearest neighbours.\n        data.edge_index = knn_graph(\n            data.x,  # x, y, z\n            k=8,\n            cosine=False,\n            loop=False\n        )\n\n        if self.y is not None:\n            y = self.y.loc[idx, :].values\n            y = torch.tensor(y, dtype=torch.float32)\n            data.y = y\n\n        return data","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:21.552511Z","iopub.execute_input":"2023-11-16T10:40:21.552892Z","iopub.status.idle":"2023-11-16T10:40:21.570826Z","shell.execute_reply.started":"2023-11-16T10:40:21.552860Z","shell.execute_reply":"2023-11-16T10:40:21.569699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing sensors data","metadata":{}},{"cell_type":"code","source":"def prepare_sensors():\n    \"\"\"\n    Preparing sensors position and quantum efficiency to be used\n    as input to model\n    \"\"\"\n    sensors = pd.read_csv(f\"{INPUT_PATH}/sensor_geometry.csv\").astype(\n        {\n            \"sensor_id\": np.int16,\n            \"x\": np.float32,\n            \"y\": np.float32,\n            \"z\": np.float32,\n        }\n    )\n    sensors[\"string\"] = 0\n    sensors[\"qe\"] = 1\n\n    # 60 DOMs per string\n    for i in range(len(sensors) // 60):\n        start, end = i * 60, (i * 60) + 60\n        sensors.loc[start:end, \"string\"] = i\n\n        # High Quantum Efficiency in the lower 50 DOMs\n        # https://arxiv.org/pdf/2209.03042.pdf (Figure 1)\n        # DeepCore located on 8 strings\n        if i in range(78, 86):\n            start_veto, end_veto = i * 60, (i * 60) + 10\n            start_core, end_core = end_veto + 1, (i * 60) + 60\n            # The DOMs deployed in DeepCore have an efficiency that is roughly\n            # a factor of 1.35 times the QE of standard DOMs\n            sensors.loc[start_core:end_core, \"qe\"] = 1.35\n\n    # https://github.com/graphnet-team/graphnet/blob/b2bad25528652587ab0cdb7cf2335ee254cfa2db/src/graphnet/models/detector/icecube.py#L33-L41\n    # Assume that \"rde\" (relative dom efficiency) is equivalent to QE\n    sensors[\"x\"] /= 500\n    sensors[\"y\"] /= 500\n    sensors[\"z\"] /= 500\n    sensors[\"qe\"] -= 1.25\n    sensors[\"qe\"] /= 0.25\n\n    return sensors","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:21.572237Z","iopub.execute_input":"2023-11-16T10:40:21.572705Z","iopub.status.idle":"2023-11-16T10:40:21.587323Z","shell.execute_reply.started":"2023-11-16T10:40:21.572650Z","shell.execute_reply":"2023-11-16T10:40:21.586265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{INPUT_PATH}/train_meta.parquet\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:21.588842Z","iopub.execute_input":"2023-11-16T10:40:21.589335Z","iopub.status.idle":"2023-11-16T10:40:21.603377Z","shell.execute_reply.started":"2023-11-16T10:40:21.589295Z","shell.execute_reply":"2023-11-16T10:40:21.602150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sensors = prepare_sensors()\nsensors","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:21.604528Z","iopub.execute_input":"2023-11-16T10:40:21.605147Z","iopub.status.idle":"2023-11-16T10:40:21.694548Z","shell.execute_reply.started":"2023-11-16T10:40:21.605115Z","shell.execute_reply":"2023-11-16T10:40:21.693498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta = pd.read_parquet(\n   f\"{INPUT_PATH}/train_meta.parquet\", columns=[\"batch_id\", \"event_id\", \"azimuth\", \"zenith\"]\n).astype(_dtype)\nmeta","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:21.695798Z","iopub.execute_input":"2023-11-16T10:40:21.696101Z","iopub.status.idle":"2023-11-16T10:40:45.064229Z","shell.execute_reply.started":"2023-11-16T10:40:21.696070Z","shell.execute_reply":"2023-11-16T10:40:45.062521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ids = meta[\"batch_id\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:45.065927Z","iopub.execute_input":"2023-11-16T10:40:45.066348Z","iopub.status.idle":"2023-11-16T10:40:46.021278Z","shell.execute_reply.started":"2023-11-16T10:40:45.066305Z","shell.execute_reply":"2023-11-16T10:40:46.020325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function","metadata":{}},{"cell_type":"code","source":"class DifferentiableClamp(torch.autograd.Function):\n    \"\"\"\n    In the forward pass this operation behaves like torch.clamp.\n    But in the backward pass its gradient is 1 everywhere, as if instead of clamp one had used the identity function.\n    \"\"\"\n    @staticmethod\n    def forward(ctx, x, min_val, max_val):\n        return x.clamp(min_val, max_val)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # None needed because of the optional arguments min_val and max v_val\n        return grad_output.clone(), None, None\n\nclass AngularLoss(pl.LightningModule):\n    \"\"\"\n    Evaluating mean angular error between the predicted and true event origins\n    \"\"\"\n    def __init__(self, eps = .001): #gradients explode, so have to add eps\n        super().__init__()\n        self.high =1-eps\n        self.low = -1+eps\n        self.Clamp = DifferentiableClamp()\n   \n    def forward(self, y_pred, y_true):\n        y_true = angles_to_unit_vector(y_true[:,0], y_true[:,1])\n        y_pred = angles_to_unit_vector(y_pred[:,0], y_pred[:,1])\n        \n        scalar_prod = torch.sum(y_pred*y_true,dim = 1)\n        scalar_prod = self.Clamp.apply(scalar_prod, self.low, self.high)\n        return torch.mean(torch.abs(torch.arccos(scalar_prod)))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:46.022571Z","iopub.execute_input":"2023-11-16T10:40:46.022979Z","iopub.status.idle":"2023-11-16T10:40:46.034444Z","shell.execute_reply.started":"2023-11-16T10:40:46.022926Z","shell.execute_reply":"2023-11-16T10:40:46.033348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def angles_to_unit_vector(azimuth, zenith):\n    \"\"\"\n    Transforms azimuth and zenith angles to unit vector in x,y,z coordinates\n    \"\"\"\n    return torch.stack([\n        torch.cos(azimuth) * torch.sin(zenith),\n        torch.sin(azimuth) * torch.sin(zenith),\n        torch.cos(zenith)\n    ], dim=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:46.035952Z","iopub.execute_input":"2023-11-16T10:40:46.036371Z","iopub.status.idle":"2023-11-16T10:40:46.168661Z","shell.execute_reply.started":"2023-11-16T10:40:46.036327Z","shell.execute_reply":"2023-11-16T10:40:46.167441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def angular_dist_score(y_pred, y_true):\n    \"\"\"\n    Competition metric used for validation\n    \"\"\"\n    y_pred = y_pred.cpu()\n    y_true = y_true.cpu()\n    az_true = y_true[:,0]\n    az_pred = y_pred[:,0]\n    zen_true = y_true[:,1]\n    zen_pred = y_pred[:,1]\n    #if not (np.all(np.isfinite(az_true)) and\n    #        np.all(np.isfinite(zen_true)) and\n    #        np.all(np.isfinite(az_pred)) and\n    #        np.all(np.isfinite(zen_pred))):\n    #    raise ValueError(\"All arguments must be finite\")\n    sa1 = np.sin(az_true)\n    ca1 = np.cos(az_true)\n    sz1 = np.sin(zen_true)\n    cz1 = np.cos(zen_true)\n    sa2 = np.sin(az_pred)\n    ca2 = np.cos(az_pred)\n    sz2 = np.sin(zen_pred)\n    cz2 = np.cos(zen_pred)\n    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n    scalar_prod =  np.clip(scalar_prod, -1, 1)\n    return np.average(np.abs(np.arccos(scalar_prod)))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:46.169982Z","iopub.execute_input":"2023-11-16T10:40:46.170293Z","iopub.status.idle":"2023-11-16T10:40:46.180257Z","shell.execute_reply.started":"2023-11-16T10:40:46.170262Z","shell.execute_reply":"2023-11-16T10:40:46.179206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graph Summary","metadata":{}},{"cell_type":"code","source":"batch = 1\nevent_ids = meta[meta[\"batch_id\"] == batch][\"event_id\"].tolist()\ny = meta[meta[\"batch_id\"] == batch][['zenith', 'azimuth']].reset_index(drop=True)\ntrain_dataset = IceCubeDataset(batch, event_ids, sensors, mode='train', y=y)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:46.181563Z","iopub.execute_input":"2023-11-16T10:40:46.181963Z","iopub.status.idle":"2023-11-16T10:40:52.059548Z","shell.execute_reply.started":"2023-11-16T10:40:46.181934Z","shell.execute_reply":"2023-11-16T10:40:52.058614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print()\nprint(f'Dataset: {train_dataset}:')\nprint('====================')\nprint(f'Number of graphs: {len(train_dataset)}')\nprint(f'Number of features: {train_dataset.num_features}')\n\ntrain_data = train_dataset[30]  # Get the first graph object.\n\nprint()\nprint(train_data)\nprint('=============================================================')\n\n# Gather some statistics about the first graph.\nprint(f'Number of nodes: {train_data.num_nodes}')\nprint(f'Number of edges: {train_data.num_edges}')\nprint(f'Average node degree: {train_data.num_edges / train_data.num_nodes:.2f}')\nprint(f'Has isolated nodes: {train_data.has_isolated_nodes()}')\nprint(f'Has self-loops: {train_data.has_self_loops()}')\nprint(f'Is undirected: {train_data.is_undirected()}')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:52.060873Z","iopub.execute_input":"2023-11-16T10:40:52.061223Z","iopub.status.idle":"2023-11-16T10:40:53.205774Z","shell.execute_reply.started":"2023-11-16T10:40:52.061185Z","shell.execute_reply":"2023-11-16T10:40:53.204589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nfrom torch_geometric.utils import to_networkx\n\nnxg = to_networkx(train_data)\nnx.draw(nxg, with_labels=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:40:53.207075Z","iopub.execute_input":"2023-11-16T10:40:53.207368Z","iopub.status.idle":"2023-11-16T10:41:01.069436Z","shell.execute_reply.started":"2023-11-16T10:40:53.207340Z","shell.execute_reply":"2023-11-16T10:41:01.068332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### Layers","metadata":{}},{"cell_type":"code","source":"class DynEdgeConv(EdgeConv, pl.LightningModule):\n    \"\"\"\n    Dynamical edge convolution layer.\n    \"\"\"\n\n    def __init__(self, nn, aggr = \"max\", nb_neighbors = 8, features_subset = None, **kwargs,):\n        \"\"\"\n        Construct `DynEdgeConv`.\n        Args:\n            nn: The MLP/torch.Module to be used within the `EdgeConv`.\n            aggr: Aggregation method to be used with `EdgeConv`.\n            nb_neighbors: Number of neighbours to be clustered after the\n                `EdgeConv` operation.\n            features_subset: Subset of features in `Data.x` that should be used\n                when dynamically performing the new graph clustering after the\n                `EdgeConv` operation. Defaults to all features.\n            **kwargs: Additional features to be passed to `EdgeConv`.\n        \"\"\"\n        # Check(s)\n        if features_subset is None:\n            features_subset = slice(None)  # Use all features\n        assert isinstance(features_subset, (list, slice))\n\n        # Base class constructor\n        super().__init__(nn=nn, aggr=aggr, **kwargs)\n\n        # Additional member variables\n        self.nb_neighbors = nb_neighbors\n        self.features_subset = features_subset\n\n    def forward(self, x, edge_index, batch = None):\n        \"\"\"\n        Forward pass.\n        \"\"\"\n        # Standard EdgeConv forward pass\n        x = super().forward(x, edge_index)\n\n        # Recompute adjacency\n        edge_index = knn_graph(\n            x=x[:, self.features_subset],\n            k=self.nb_neighbors,\n            batch=batch,\n        ).to(self.device)\n\n        return x, edge_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Global Variables","metadata":{}},{"cell_type":"code","source":"def calculate_xyzt_homophily(x, edge_index, batch):\n    \"\"\"\n    Calculate xyzt-homophily from a batch of graphs.\n\n    Homophily is a graph scalar quantity that measures the likeness of\n    variables in nodes. Notice that this calculator assumes a special order of\n    input features in x.\n\n    Returns:\n        Tuple, each element with shape [batch_size,1].\n    \"\"\"\n    hx = homophily(edge_index, x[:, 0], batch).reshape(-1, 1)\n    hy = homophily(edge_index, x[:, 1], batch).reshape(-1, 1)\n    hz = homophily(edge_index, x[:, 2], batch).reshape(-1, 1)\n    ht = homophily(edge_index, x[:, 3], batch).reshape(-1, 1)\n    return hx, hy, hz, ht","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_global_variables(x, edge_index, batch, *additional_attributes):\n        \"\"\"\n        Calculate global variables.\n        \"\"\"\n        # Calculate homophily (scalar variables)\n        h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n\n        # Calculate mean features\n        global_means = scatter_mean(x, batch, dim=0)\n\n        # Add global variables\n        global_variables = torch.cat(\n            [\n                global_means,\n                h_x,\n                h_y,\n                h_z,\n                h_t,\n            ],\n            dim=1,\n        )\n\n        return global_variables","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building model","metadata":{}},{"cell_type":"code","source":"class EdgeModel(torch.nn.Module):\n    def __init__(self, dim_h):\n        super(EdgeModel, self).__init__()\n        self.n_features = train_dataset.num_features\n        self.n_outputs = 2\n        self.convolution = nn.ModuleList([DynEdgeConv(nn.Sequential(Linear(2*self.n_features, 128), nn.LeakyReLU(),\n                                             Linear(128, 256), nn.LeakyReLU())),\n                                         DynEdgeConv(nn.Sequential(Linear(512, 336), nn.LeakyReLU(),\n                                             Linear(336, 256), nn.LeakyReLU())),\n                                         DynEdgeConv(nn.Sequential(Linear(512, 336), nn.LeakyReLU(),\n                                             Linear(336, 256), nn.LeakyReLU()))])\n        self.post_process = nn.Sequential(Linear(774, 336), nn.LeakyReLU(),\n                                          Linear(336, 256), nn.LeakyReLU())\n        self.readout = nn.Sequential(Linear(778, 128), nn.LeakyReLU())\n        self.out = nn.Sequential(Linear(128, self.n_outputs),\n                                      BatchNorm1d(self.n_outputs), nn.LeakyReLU())\n        \n    def forward(self, x, edge_index, batch):\n        # 1. Obtain node embeddings\n        global_features = calculate_global_variables(x, \n                                                     edge_index,\n                                                     batch,\n                                                    )\n        \n        graphs = [x]\n        for i, layer in enumerate(self.convolution):\n            graph = layer(graphs[i], edge_index)\n            graphs.append(graph)\n        x = torch.cat(graphs, dim=1)\n        \n        x = self.post_process(x)\n        \n        pool_x = []\n        for pool in GLOBAL_POOLINGS.values():\n            pool_x.append(pool(x, batch))\n        \n        pool_x.append(global_features)\n        pool_x = torch.cat(pool_x, dim=1)\n        \n        x = self.readout(pool_x)\n        x = self.out(x)\n        return x\n    \n    def fit(self, loader, batch_size=256, epochs=5, device='cuda'):\n        loss_fn = AngularLoss()\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)  \n        self.train()\n        \n        for epoch in range(epochs+1):\n            epoch_loss = 0\n            for batch in loader:\n                batch = batch.to(device)\n                optimizer.zero_grad()\n                out = self(batch.x, batch.edge_index, batch.batch)\n                loss = loss_fn(out, batch.y.reshape(out.size()))\n                epoch_loss += loss.item()\n                loss.backward()\n                optimizer.step()   \n            epoch_loss = epoch_loss / len(loader)\n            print(f'Epoch: {epoch}, Loss: {epoch_loss:.4f}')\n\n    @torch.no_grad()\n    def validate(self, data, batch_size=256, device='cuda'):\n        loss_fn = angular_dist_score\n        epoch_loss = 0\n        predictions = []\n        \n        self.eval()\n        for batch in loader:\n            batch = batch.to(device)\n            \n            out = self(batch.x, batch.edge_index, batch.batch)\n                \n            loss = loss_fn(out, batch.y.reshape(out.size()))\n                \n            epoch_loss += loss.item()\n            \n            predictions.append(out)\n                \n        epoch_loss = epoch_loss / len(loader)\n        print(f'Loss: {epoch_loss:.4f}')\n        return torch.cat(predictions, 0)\n    \n    @torch.no_grad()\n    def test(self, data, device='cuda'):\n        loader = DataLoader(data, batch_size=128, shuffle=False)\n        predictions = []\n        \n        self.eval()\n        for data in loader:\n            data.to(device)\n            out = self(data.x, data.edge_index, data.batch)\n            predictions.append(out)\n        return torch.cat(predictions, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EdgeModel(dim_h=128)\nprint(model)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('using ', device)\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"if MODE == 'train':\n    for batch in random.sample(range(1, 600), TRAIN_BATCHES):\n        i = 1\n        path = '/kaggle/working/EdgeConvV3_batch_' + str(i) + '.pth'\n        print(f'Batch: {batch:03d}')\n        print('====================')\n        event_ids = meta[meta[\"batch_id\"] == batch][\"event_id\"].tolist()\n        y = meta[meta[\"batch_id\"] == batch][['azimuth', 'zenith']].reset_index(drop=True)\n\n        train_dataset = IceCubeDataset(batch, event_ids, sensors, mode='train', y=y)\n        loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n        print(device)\n        model.fit(loader, device=device)\n        torch.save(model, path)\nelif MODE == 'resume':\n    model = torch.load(MODEL_PATH)\n    for batch in random.sample(range(1, 600), TRAIN_BATCHES):\n        i = 1\n        path = '/kaggle/working/EdgeConvV3_batch_' + str(i) + '.pth'\n        print(f'Batch: {batch:03d}')\n        print('====================')\n        event_ids = meta[meta[\"batch_id\"] == batch][\"event_id\"].tolist()\n        y = meta[meta[\"batch_id\"] == batch][['azimuth', 'zenith']].reset_index(drop=True)\n\n        train_dataset = IceCubeDataset(batch, event_ids, sensors, mode='train', y=y)\n        loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n        print(device)\n        model.fit(loader, device=device)\n        torch.save(model, path)\nelse:\n    model = torch.load(MODEL_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/EdgeConvV6.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"markdown","source":"## TODO\nvalidate using competition metric","metadata":{}},{"cell_type":"code","source":"batch = random.randint(601, 660)\nprint(batch)\n\nevent_ids = meta[meta[\"batch_id\"] == batch][\"event_id\"].tolist()\ny = meta[meta[\"batch_id\"] == batch][['azimuth', 'zenith']].reset_index(drop=True)\n\nval_dataset = IceCubeDataset(batch, event_ids, sensors, mode='train', y=y)\nloader = DataLoader(val_dataset, batch_size=512, shuffle=False)\noutput = model.validate(loader, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = {\n    \"event_id\": event_ids,\n    \"azimuth\": output[:, 0].cpu(),# % (2 * math.pi),\n    \"zenith\": output[:, 1].cpu(), #% math.pi,\n}\n\nsub = pd.DataFrame(sub)\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Infering","metadata":{}},{"cell_type":"code","source":"test_meta = pd.read_parquet(\n    f\"{INPUT_PATH}/test_meta.parquet\", columns=[\"batch_id\", \"event_id\"]\n).astype(_dtype)\ntest_meta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"event_id_labels = []\nbatch_preds = []\nbatch_ids = test_meta[\"batch_id\"].unique()\nfor batch in batch_ids:\n    test_event_ids = test_meta[test_meta[\"batch_id\"] == batch][\"event_id\"].tolist()\n    test_dataset = IceCubeDataset(batch, test_event_ids, sensors, mode='test', y=None)\n    # .cpu().detach()\n    batch_preds.append(model.test(test_dataset))\n    event_id_labels.extend(test_meta[test_meta[\"batch_id\"] == batch][\"event_id\"].tolist())\n    print(\"Finished batch\", batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making Submission","metadata":{}},{"cell_type":"code","source":"import math\n\noutput = torch.cat(batch_preds, 0).cpu()\nsub = {\n    \"event_id\": event_id_labels,\n    \"azimuth\": output[:, 0],# % (2 * math.pi),\n    \"zenith\": output[:, 1], #% math.pi,\n}\n\nsub = pd.DataFrame(sub)\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}